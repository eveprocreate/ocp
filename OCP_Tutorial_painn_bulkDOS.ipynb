{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Open-Catalyst-Project/ocp/blob/tutorials_01_11/tutorials/OCP_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_imports() error raised, continue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/users/elizaveta.starykh/conda_envs/python9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/l/users/elizaveta.starykh/OCP_project/ocp-git/\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# print(sys.version)\n",
    "import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import ase.io\n",
    "from ase.io.trajectory import Trajectory\n",
    "from ase.io import extxyz\n",
    "from ase.calculators.emt import EMT\n",
    "from ase.build import fcc100, add_adsorbate, molecule\n",
    "from ase.constraints import FixAtoms\n",
    "from ase.optimize import LBFGS\n",
    "from ase.visualize.plot import plot_atoms\n",
    "from ase import Atoms\n",
    "from IPython.display import Image\n",
    "\n",
    "import ocpmodels\n",
    "import lmdb\n",
    "import torch_geometric\n",
    "\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "\n",
    "### May 2024, python9_kernel, python9 env, python=3.9.18, CSCC\n",
    "import e3nn\n",
    "from ocpmodels.trainers import OCPTrainer\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from ocpmodels import models\n",
    "from ocpmodels.common import logger\n",
    "from ocpmodels.common.utils import setup_logging, setup_imports\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        setup_imports()\n",
    "        break\n",
    "    except (ModuleNotFoundError, RuntimeError, TypeError, NameError):\n",
    "        print('setup_imports() error raised, continue...')\n",
    "        pass\n",
    "\n",
    "setup_logging()\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch_geometric.loader  \n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "import pickle as pkl\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dataset_config.yaml\", 'r') as file:\n",
    "    dataset_info = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_config = dataset_info[\"datasets\"][1]\n",
    "val_dataset_config = dataset_info[\"datasets\"][2]\n",
    "\n",
    "train_src = train_dataset_config[\"path\"]\n",
    "val_src = val_dataset_config[\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# /configs/oc22/s2ef/painn\n",
    "\n",
    "with open('configs/oc22/s2ef/painn/painn.yml', 'r') as file:\n",
    "    model = yaml.safe_load(file)\n",
    "\n",
    "model = model[\"model\"]\n",
    "model['name'] = 'ocpmodels.models.painn.painn.PaiNN'\n",
    "model['hidden_channels'] = 512\n",
    "model['efermi_length'] = 128\n",
    "if train_dataset_config[\"efermi_available\"]:    \n",
    "    model[\"multiply_efermi\"] = False\n",
    "    model[\"concatenate_efermi\"] = True\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task\n",
    "task = {\n",
    "    'dataset': 'lmdb', # dataset used for the S2EF task\n",
    "    'description': 'Regressing to energies and forces for DFT trajectories from OCP',\n",
    "    'type': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'labels': ['potential energy'],\n",
    "    'grad_input': 'atomic forces',\n",
    "    'train_on_free_atoms': True,\n",
    "    'eval_on_free_atoms': True\n",
    "}\n",
    "\n",
    "# Optimizer\n",
    "optimizer = {\n",
    "    'batch_size': 32,         # originally 32\n",
    "    'eval_batch_size': 32,    # originally 32\n",
    "    'num_workers': 2,\n",
    "    'lr_initial': 5.e-4,\n",
    "    'optimizer': 'AdamW',\n",
    "    'optimizer_params': {\"amsgrad\": True},\n",
    "    'scheduler': \"ReduceLROnPlateau\",\n",
    "    'mode': \"min\",\n",
    "    'factor': 0.8,\n",
    "    'patience': 3,\n",
    "    'max_epochs': 1,         # used for demonstration purposes\n",
    "    'force_coefficient': 100,\n",
    "    'ema_decay': 0.999,\n",
    "    'clip_grad_norm': 10,\n",
    "    'loss_energy': 'mae',\n",
    "    'loss_force': 'l2mae',\n",
    "    # 'eval_every': 500\n",
    "}\n",
    "# Dataset\n",
    "dataset = [\n",
    "{'src': train_src,\n",
    "'normalize_labels': True,\n",
    "#  \"target_mean\": mean,\n",
    "\"target_mean\": train_dataset_config[\"mean\"],\n",
    "#  \"target_std\": stdev,\n",
    "\"target_std\": train_dataset_config[\"stdev\"],\n",
    "\"grad_target_mean\": 0.0,\n",
    "#  \"grad_target_std\": stdev\n",
    "\"grad_target_std\": train_dataset_config[\"stdev\"]\n",
    "}, # train set\n",
    "{'src': val_src}, # val set (optional)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_wandb(config=None):\n",
    "#     with wandb.init(project=\"PaiNN-DOS-efermi\", config = sweep_configuration):\n",
    "#         config = wandb.config\n",
    "\n",
    "#         for epoch in range(config.epochs):\n",
    "\n",
    "#             print(wandb.config[\"batch_size\"])\n",
    "#             batch_size = config['batch_size']\n",
    "\n",
    "#             task, optimizer, dataset = task_optim_dataset(batch_size)\n",
    "\n",
    "\n",
    "#             trainer = OCPTrainer(\n",
    "#                 task=task,\n",
    "#                 model=copy.deepcopy(model), # copied for later use, not necessary in practice.\n",
    "#                 dataset=dataset,\n",
    "#                 optimizer=optimizer,\n",
    "#                 outputs={},\n",
    "#                 loss_fns={},\n",
    "#                 eval_metrics={},\n",
    "#                 name=\"s2ef\",\n",
    "#                 identifier=\"S2EF-example\",\n",
    "#                 run_dir=\".\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "#                 is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "#                 print_every=5,\n",
    "#                 seed=0, # random seed to use\n",
    "#                 logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "#                 local_rank=0,\n",
    "#                 amp=True, # use PyTorch Automatic Mixed Precision (faster training and less memory usage),\n",
    "#             )\n",
    "\n",
    "\n",
    "\n",
    "#             trainer.train()\n",
    "#             avg_loss = trainer.final_loss\n",
    "#             wandb.log({\"loss\": avg_loss, \"epoch\": epoch}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "trainer = OCPTrainer(\n",
    "    task=task,\n",
    "    model=copy.deepcopy(model), # copied for later use, not necessary in practice.\n",
    "    dataset=dataset,\n",
    "    optimizer=optimizer,\n",
    "    outputs={},\n",
    "    loss_fns={},\n",
    "    eval_metrics={},\n",
    "    name=\"s2ef\",\n",
    "    identifier=\"S2EF-20k-train\",\n",
    "    run_dir=\".\", # directory to save results if is_debug=False. Prediction files are saved here so be careful not to override!\n",
    "    is_debug=False, # if True, do not save checkpoint, logs, or results\n",
    "    print_every=5,\n",
    "    seed=0, # random seed to use\n",
    "    logger=\"tensorboard\", # logger of choice (tensorboard and wandb supported)\n",
    "    local_rank=0,\n",
    "    amp=True, # use PyTorch Automatic Mixed Precision (faster training and less memory usage),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit(\"finishing trainer.train() function, exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean, std [feel free to skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5156444102461508, 0.45158625849998374)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset = LmdbDataset({\"src\": \"./data/s2ef/200k/train/output_lmdb\"})\n",
    "dataset = LmdbDataset({\"src\": \"./tutorial_data/s2ef/train_100\"})\n",
    "\n",
    "energies = []\n",
    "for data in dataset:\n",
    "    energies.append(data.y)\n",
    "\n",
    "mean = np.mean(energies)\n",
    "stdev = np.std(energies)\n",
    "\n",
    "stdev, mean \n",
    "## == (2.8471219290033876, -0.7877437496095779) for /normalized_efermi/ 176k dataset\n",
    "## == (2.8412495666979143, -0.78793442576679) for ??\n",
    "## == (2.8392264933285123, -0.7946160068500009) for 20k dataset\n",
    "# (2.8873626757898343, -0.7555769032155011) for 200k train dataset\n",
    "# == (1.5156444102461508, 0.45158625849998374) for tutorial_data/train_100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176027"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/dataset_config.yaml\", 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'path', 'mean', 'stdev', 'efermi_available', 'description'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_data[\"datasets\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200k_train'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# yaml_data\n",
    "# yaml_data[\"datasets\"][1][\"description\"]\n",
    "\n",
    "\n",
    "selected_dataset_info = yaml_data[\"datasets\"][0]\n",
    "selected_dataset_info[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (2.8873626757898343, -0.7555769032155011) for 200k train dataset\n",
    "dataset_1 = {'name': \"200k_train\",\n",
    "             'path': \"/l/users/elizaveta.starykh/OCP_project/ocp-git/data/s2ef/200k/train/\", \n",
    "             \"mean\": -0.7555769032155011, \n",
    "             \"stdev\": 2.8873626757898343,\n",
    "             \"efermi_available\": False}\n",
    "\n",
    "## stdev, mean == (2.8471219290033876, -0.7877437496095779) for /normalized_efermi/ 176k dataset\n",
    "dataset_2 = {'name': \"176k_train\",\n",
    "             'path': \"/l/users/elizaveta.starykh/OCP_project/ocp-git/data/s2ef/200k/train/output_lmdb/normalized_efermi\", \n",
    "             \"mean\": -0.7877437496095779, \n",
    "             \"stdev\": 2.8471219290033876,\n",
    "             \"efermi_available\": True}\n",
    "\n",
    "## == (2.8392264933285123, -0.7946160068500009) for 20k dataset\n",
    "dataset_3 = {'name': \"20k_train\",\n",
    "             'path': \"/l/users/elizaveta.starykh/OCP_project/ocp-git/data/s2ef/200k/train/output_lmdb/normalized_efermi/train_20000_systems\", \n",
    "             \"mean\": -0.7946160068500009, \n",
    "             \"stdev\": 2.8392264933285123,\n",
    "             \"efermi_available\": True}\n",
    "\n",
    "## == (1.5156444102461508, 0.45158625849998374) for \n",
    "dataset_4 = {'name': \"tutorial_train\",\n",
    "             'path': \"/l/users/elizaveta.starykh/OCP_project/ocp-git/tutorial_data/s2ef/train_100\", \n",
    "             \"mean\": 0.45158625849998374, \n",
    "             \"stdev\": 1.5156444102461508,\n",
    "             \"efermi_available\": False}\n",
    "\n",
    "dataset_conf = dict{\"dataset\" : dt for }\n",
    "\n",
    "\n",
    "with open('./data/dataset_config.yml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = LmdbDataset({\"src\": \"./data/s2ef/200k/train/output_lmdb\"})\n",
    "\n",
    "efermi_info = []\n",
    "# for data in train_dataset:\n",
    "for data in dataset:\n",
    "    efermi_info.append(data.efermi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1.0 / (1 + np.exp(-z)) \n",
    "\n",
    "def tanh(z):\n",
    "\treturn (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "efermi_info = np.array(efermi_info)\n",
    "efermi_tanh = tanh(efermi_info)\n",
    "\n",
    "\n",
    "########\n",
    "# efermi_normalized = (efermi_info - efermi_info.min()) / (efermi_info.max() - efermi_info.min()) \n",
    "\n",
    "# efermi_normalized.min(), efermi_normalized.max(), efermi_normalized.std()\n",
    "# efermi_sigmoid = sigmoid(efermi_info)\n",
    "\n",
    "# mean = np.mean(efermi_info)\n",
    "# stdev = np.std(efermi_info)\n",
    "\n",
    "# stdev, mean \n",
    "## == (2.8471219290033876, -0.7877437496095779) for /output_lmdb/ full dataset\n",
    "## == (2.8412495666979143, -0.78793442576679)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efermi_tanh.min(), efermi_tanh.max(), efermi_tanh.std()\n",
    "len(efermi_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(efermi_info, efermi_tanh, 'bo')\n",
    "# plt.yscale(\"log\")\n",
    "plt.xlabel(\"efermi\")\n",
    "\n",
    "plt.ylabel(\"efermi tanh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = LmdbDataset({\"src\": \"./data/s2ef/200k/train/output_lmdb/\"})\n",
    "\n",
    "# tst_efermi = torch.FloatTensor([train_dataset[9].efermi])\n",
    "# tst_efermi.dim(), tst_efermi.shape\n",
    "# tst_efermi\n",
    "\n",
    "values_efermi = [6.7264, 8.2544, 1.5132]\n",
    "\n",
    "long_efermi = torch.FloatTensor([tst_dataset[0].efermi, tst_dataset[5].efermi, tst_dataset[7].efermi])\n",
    "long_efermi.dim(), long_efermi.shape\n",
    "long_efermi\n",
    "\n",
    "\n",
    "\n",
    "batch_efermi = [torch.FloatTensor([tst_dataset[x].efermi]) for x in range(3)]\n",
    "\n",
    "batch_efermi\n",
    "\n",
    "\n",
    "# tst_dataset[9].efermi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "rand_tensor = torch.rand([3, 4])\n",
    "rand_tensor.shape # = [3, 4]\n",
    "## embedding.shape = [153, 512]  ### 153 elements, each has a vactor of size 512\n",
    "\n",
    "\n",
    "float_number = -1.1\n",
    "\n",
    "# rand_tensor[1] = torch.tensor([1, 2, 3, 4])\n",
    "# rand_tensor[1] = torch.mul(rand_tensor[1], float_number)\n",
    "# rand_tensor[1], rand_tensor \n",
    "\n",
    "obj = torch.mul(rand_tensor[:2], float_number)\n",
    "obj.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of `efermi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_lmdb = LmdbDataset({\"src\": \"./data/s2ef/200k/train/output_lmdb/\"})\n",
    "os.makedirs(\"./data/s2ef/200k/train/output_lmdb/normalized_efermi\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset), len(efermi_tanh), train_dataset[0].efermi, efermi_tanh[0] \n",
    "len(dataset), len(efermi_tanh), dataset[0].efermi, efermi_tanh[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lmdb.open(\n",
    "    \"./data/s2ef/200k/train/output_lmdb/normalized_efermi/train_data_200k_efermi.lmdb\",\n",
    "    map_size=1099511627776 * 2,\n",
    "    subdir=False,\n",
    "    meminit=False,\n",
    "    map_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sys_nr=0\n",
    "# no_dos_count=0\n",
    "\n",
    "# for system_nr, system in tqdm(enumerate(train_dataset), total=10):\n",
    "for system_nr, system in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    \n",
    "    if system_nr%100==0:\n",
    "        print(\"running nr: \", system_nr, \"\\tcorrect system number: \", correct_sys_nr)\n",
    "\n",
    "    system.efermi = efermi_tanh[system_nr]\n",
    "\n",
    "\n",
    "    # system_bulk_id = log_ids.loc[system_nr, \"bulk_mpid\"]\n",
    "    \n",
    "    # if system_bulk_id in unique_dos_postprocessed.keys():\n",
    "        \n",
    "\n",
    "\n",
    "    #     full_system_dos = unique_dos_postprocessed[system_bulk_id]\n",
    "    #     system.energies = full_system_dos[0]\n",
    "    #     system.efermi = full_system_dos[1]\n",
    "    #     system.bulk_total_dos = full_system_dos[2]\n",
    "    \n",
    "    \n",
    "    # else:\n",
    "    #     no_dos_count+=1\n",
    "    #     continue\n",
    "#     #### adding to the new lmdb file:\n",
    "    \n",
    "    txn = db.begin(write=True)\n",
    "    txn.put(f\"{correct_sys_nr}\".encode(\"ascii\"), pkl.dumps(system, protocol=-1))\n",
    "    txn.commit()\n",
    "    db.sync()\n",
    "    correct_sys_nr+=1\n",
    "\n",
    "db.sync()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = LmdbDataset({\"src\": \"./data/s2ef/200k/train/output_lmdb/normalized_efermi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dta), dta[10].efermi, dta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PoF-BxSM5Jkc",
    "bSt6h_Q-oqjK",
    "pto2SpJPwlz1",
    "gaauxWdNw_-4",
    "TcUvAI81xoSt",
    "TUH5BaaXo-ca"
   ],
   "include_colab_link": true,
   "name": "CCAI - OCP Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python9_kernel",
   "language": "python",
   "name": "python9_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
